{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pandas import DataFrame\n",
    "from functools import reduce\n",
    "from enum import Enum\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alexandermendelsohn/Documents/Sales Analysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging a 12 Months of Sales Data into a Single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv(\"Sales Data/Sales_April_2019.csv\")\n",
    "file=[ i for i in os.listdir(\"Sales Data\")]\n",
    "df=pd.DataFrame()\n",
    "for j in file:\n",
    "    print(\"Sales Data/\"+j)\n",
    "    x=pd.read_csv(\"Sales Data/\"+j)\n",
    "    df=pd.concat([df,x])\n",
    "    \n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Data into new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Updated_sales.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define function to read data in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = df.columns.tolist()\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df[all_features]).sum(axis=0).sort_values(ascending=False)[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.drop(['Unnamed: 0'], axis=1)\n",
    "df_tmp[df_tmp.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_vars = (df.dtypes.values == np.dtype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"Updated_sales.csv\"\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "def read_raw_data(file_path: str, chunk_size: int=1000) -> DataFrame:\n",
    "    csv_reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    processed_chunks = []\n",
    "\n",
    "    # append the processed chunk to the list\n",
    "    for chunk in csv_reader:\n",
    "        chunk = chunk.drop(['Unnamed: 0'], axis=1)\n",
    "        chunk = chunk.drop_duplicates()\n",
    "        chunk = chunk.loc[chunk[\"Order ID\"] != \"Order ID\"].dropna()\n",
    "        processed_chunks.append(chunk)\n",
    "\n",
    "    # concatenate the processed chunks into a single DataFrame\n",
    "    return pd.concat(processed_chunks, axis=0)\n",
    "\n",
    "df_prepro = read_raw_data(file_path=FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_purchase_address(df_to_process: DataFrame) -> DataFrame:\n",
    "    df_address_split = df_to_process[\"Purchase Address\"].str.split(\",\", n=3, expand=True)\n",
    "    df_address_split.columns = [\"Street Name\", \"City\", \"State and Postal Code\"]\n",
    "    \n",
    "    df_state_postal_split = (\n",
    "        df_address_split[\"State and Postal Code\"]\n",
    "        .str.strip()\n",
    "        .str.split(\" \", n=2, expand=True)\n",
    "    )\n",
    "    df_state_postal_split.columns = [\"State Code\", \"Postal Code\"]\n",
    "    \n",
    "    return pd.concat([df_to_process, df_address_split, df_state_postal_split], axis=1)\n",
    "\n",
    "def split_order_date(df_to_process: DataFrame) -> DataFrame:\n",
    "    df_to_process['Order Month'] = pd.to_datetime(df_to_process['Order Date']).dt.month\n",
    "    df_to_process['Order Day'] = pd.to_datetime(df_to_process['Order Date']).dt.day\n",
    "    df_to_process['Order Hour'] = pd.to_datetime(df_to_process['Order Date']).dt.hour\n",
    "    df_to_process['Order Year'] = pd.to_datetime(df_to_process['Order Date']).dt.year\n",
    "    \n",
    "    return df_to_process\n",
    "\n",
    "def convert_numerical_column_types(df_to_process: DataFrame) -> DataFrame:\n",
    "    df_to_process[\"Quantity Ordered\"] = df_to_process[\"Quantity Ordered\"].astype(int)\n",
    "    df_to_process[\"Price Each\"] = df_to_process[\"Price Each\"].astype(float)\n",
    "    df_to_process[\"Order ID\"] = df_to_process[\"Order ID\"].astype(int)\n",
    "    \n",
    "    return df_to_process\n",
    "\n",
    "\n",
    "def calculate_total_order_cost(df_to_process: DataFrame) -> DataFrame:\n",
    "    df_to_process[\"Total Cost\"] = df_to_process[\"Quantity Ordered\"] * df_to_process[\"Price Each\"]\n",
    "    return df_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = (\n",
    "    df\n",
    "    .pipe(split_purchase_address)\n",
    "    .pipe(split_order_date)\n",
    "    .pipe(convert_numerical_column_types)\n",
    "    .pipe(calculate_total_order_cost)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping features sets as ENUM\n",
    "\n",
    "An enum, short for enumeration, is a “set of symbolic names (members) bound to unique values”. Enums have a few key benefits:\n",
    "\n",
    "- Defining enums lets you have related constants organized in one (or many) classes that can act as a source of truth for dimensions, measures, and other constants you need to call in your pipelines;\n",
    "- Using enums will allow you to avoid passing invalid values in your data pipelines, assuming you correctly define and maintain the enum class;\n",
    "- Enums allow users to work with a standardized set of data points and constants, which is helpful when multiple people are aggregating or creating models based on one main source of data (to help avoid having multiple definitions or aliases for the same column in the raw data source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalColumns(Enum):\n",
    "    PRODUCT = \"Product\"\n",
    "    QUANTITY_ORDERED = \"Quantity Ordered\"\n",
    "    CITY = \"City\"\n",
    "    STATE_CODE = \"State Code\"\n",
    "    POSTAL_CODE = \"Postal Code\"\n",
    "    HOUSE_NUMBER = \"House Number\"\n",
    "    ORDER DAY = \"Order Month\"\n",
    "    ORDER MONTH = \"Order Day\"\n",
    "    ORDER HOUR = \"Order Hour\"\n",
    "    ORDER YEAR = \"Order Year\"\n",
    "    \n",
    "class AddressColumns(Enum):\n",
    "    STREET_NAME = \"Street Name\"\n",
    "    CITY = \"City\"\n",
    "    STATE_CODE = \"State Code\"\n",
    "    POSTAL_CODE = \"Postal Code\"\n",
    "    \n",
    "class NumericalColumns(Enum):\n",
    "    TOTAL_COST = \"Total Cost\"\n",
    "    QUANTITY_ORDERED = \"Quantity Ordered\"\n",
    "    PRICE EACH = 'Price Each'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SalesGroupByColumns.PRODUCT.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column.value for column in SalesGroupByColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_columns = [column.value for column in SalesGroupByColumns]\n",
    "\n",
    "grouped_df = (\n",
    "    processed_df\n",
    "    .groupby(groupby_columns)\n",
    "    [\"Order ID\"]\n",
    "    .count()\n",
    "    .reset_index()\n",
    "    .sort_values(\"Order ID\", ascending=False)\n",
    "    .rename({\"Order ID\": \"Count of Order IDs\"}, axis=1)\n",
    ")\n",
    "\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then separately we can do the groupby\n",
    "groupby_columns = [column.value for column in AddressColumns]\n",
    "\n",
    "grouped_df = (\n",
    "    processed_df\n",
    "    .groupby(groupby_columns)\n",
    "    .agg(\n",
    "        Total_Cost=(SalesMeasureColumns.TOTAL_COST.value, np.sum),\n",
    "        Total_Quantity_Ordered=(SalesMeasureColumns.QUANTITY_ORDERED.value, np.sum)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"Total_Cost\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_conditions = [\n",
    "    grouped_df[\"Street Name\"].str.contains(\"North\"),\n",
    "    grouped_df[\"Postal Code\"].str.contains(\"940\"),\n",
    "    grouped_df[\"Total_Cost\"] < 1000\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce method from functools allows you to pass a function and an iterable as arguments. The reduce method then applies the function to the elements in the iterable cumulatively. This means that it will perform the function in sequence for the set and combination of elements in the iterable.\n",
    "\n",
    "Defining column names and filters in a centralized place means that everyone can refer back to a single source of truth and avoid having different names and logic to refer to ultimately the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functools reduce\n",
    "grouped_df.loc[reduce(lambda x, y: x & y, filter_conditions)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
